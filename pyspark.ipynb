{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95c9cb6a-d960-47d9-b8af-15e9efcea7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/anaconda3/lib/python3.11/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/anaconda3/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b9a9d5-9776-43b2-a3b9-e3eb1ed615ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/05 18:19:49 WARN Utils: Your hostname, Vikashs-Laptop.local resolves to a loopback address: 127.0.0.1; using 192.168.1.3 instead (on interface en0)\n",
      "24/05/05 18:19:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/05 18:19:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n",
      "8\n",
      "<bound method RDD.glom of ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289>\n",
      "Help on method parallelize in module pyspark.context:\n",
      "\n",
      "parallelize(c: Iterable[~T], numSlices: Optional[int] = None) -> pyspark.rdd.RDD[~T] method of pyspark.context.SparkContext instance\n",
      "    Distribute a local Python collection to form an RDD. Using range\n",
      "    is recommended if the input represents a range for performance.\n",
      "    \n",
      "    .. versionadded:: 0.7.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    c : :class:`collections.abc.Iterable`\n",
      "        iterable collection to distribute\n",
      "    numSlices : int, optional\n",
      "        the number of partitions of the new RDD\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`RDD`\n",
      "        RDD representing distributed collection.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      "    [[0], [2], [3], [4], [6]]\n",
      "    >>> sc.parallelize(range(0, 6, 2), 5).glom().collect()\n",
      "    [[], [0], [], [2], [4]]\n",
      "    \n",
      "    Deal with a list of strings.\n",
      "    \n",
      "    >>> strings = [\"a\", \"b\", \"c\"]\n",
      "    >>> sc.parallelize(strings, 2).glom().collect()\n",
      "    [['a'], ['b', 'c']]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Create an RDD from a Python list\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Perform operations on the RDD\n",
    "result = rdd.map(lambda x: x * 2).collect()\n",
    "\n",
    "print(result)  # Output: [2, 4, 6, 8, 10]\n",
    "print(rdd.getNumPartitions()) \n",
    "# to check the number of partition  for the rdd\n",
    "print(rdd.glom)\n",
    "# it print the record from each partition we need to avoid run this if data is huge in one partition or we can do for print some lines\n",
    "\n",
    "help(sc.parallelize) \n",
    "# it helps to read about fucntion using help \n",
    "rdd.collect()\n",
    "# using collect function we can check the rdd output using list we used collect as data is not huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d1847e-665a-4007-bf27-b4600da3a27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://192.168.1.3:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Get or create a SparkContext\n",
    "spark_context = SparkContext.getOrCreate()\n",
    "\n",
    "# Access the Spark UI URL\n",
    "ui_url = spark_context.uiWebUrl\n",
    "\n",
    "print(ui_url)\n",
    "spark_context.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5701e734-5dc4-41bb-a221-f87cbd2be074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 34|\n",
      "|    Bob| 45|\n",
      "|Charlie| 29|\n",
      "|  David| 41|\n",
      "|    Eve| 38|\n",
      "+-------+---+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a list of tuples (or a list of lists) representing your data\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29), (\"David\", 41), (\"Eve\", 38)]\n",
    "\n",
    "# Create a DataFrame using the created SparkSession and the list of tuples\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "# To show the schema of data frame \n",
    "df.printSchema()\n",
    "spark.stop() \n",
    "# Do not forget to stop the spark application other wise it will give error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "765cabab-3b38-4cde-bb58-46a033ddef49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, (['A'], ['null'])), (2, (['B'], ['X'])), (3, (['C'], ['Y'])), (4, (['D'], ['null'])), (5, (['null'], ['Z']))]\n"
     ]
    }
   ],
   "source": [
    "# Import SparkContext from the pyspark module\n",
    "from pyspark import SparkContext\n",
    "\n",
    "if 'sc' in globals():\n",
    "    sc.stop()\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "# Sample data for RDD1\n",
    "rdd1_data = sc.parallelize([(1, 'A'), (2, 'B'), (3, 'C'), (4, 'D')])\n",
    "\n",
    "# Sample data for RDD2\n",
    "rdd2_data = sc.parallelize([(2, 'X'), (3, 'Y'), (5, 'Z')])\n",
    "\n",
    "# Convert RDD1 and RDD2 to key-value pair RDDs\n",
    "rdd1_kv = rdd1_data.map(lambda x: (x[0], x[1]))\n",
    "rdd2_kv = rdd2_data.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "# Perform left join using cogroup()\n",
    "joined_rdd = rdd1_kv.cogroup(rdd2_kv)\n",
    "\n",
    "# Process the joined RDD to get the desired result\n",
    "def process_join(kv):\n",
    "    key = kv[0]\n",
    "    values1 = list(kv[1][0]) if kv[1][0] else ['null']\n",
    "    values2 = list(kv[1][1]) if kv[1][1] else ['null']\n",
    "    return (key, (values1, values2))\n",
    "\n",
    "result = joined_rdd.map(process_join)\n",
    "\n",
    "# Collect the result as a list and print it\n",
    "print(result.collect())\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3b72d5b-adc8-49b5-8126-994b16257dd1",
   "metadata": {},
   "source": [
    "ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by getOrCreate at /var/folders/ht/5d19w4p161dgmlf2yzfbz7qh0000gn/T/ipykernel_88175/2416547275.py:4 \n",
    "To deal with this error we should need to stop the other spark context or session so in this case we need to use\n",
    "if 'sc' in globals():\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42d1ae67-3f09-453c-a82b-e9bfbd4a7ad4",
   "metadata": {},
   "source": [
    "join in rdd using join keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec3646bb-cc53-4b14-8001-451cbfc93269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, ('B', 'X')), (3, ('C', 'Y'))]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"JoinExample\")\n",
    "\n",
    "# Sample data for RDD1\n",
    "rdd1 = sc.parallelize([(1, 'A'), (2, 'B'), (3, 'C')])\n",
    "\n",
    "# Sample data for RDD2\n",
    "rdd2 = sc.parallelize([(2, 'X'), (3, 'Y'), (4, 'Z')])\n",
    "\n",
    "# Perform inner join on the two RDDs\n",
    "joined_rdd = rdd1.join(rdd2)\n",
    "\n",
    "# Print the result\n",
    "print(joined_rdd.collect())\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a9cbcf0-076e-4f24-a993-f0d5a78f3700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Join Result: [(2, ('B', 'X')), (1, ('A', None)), (3, ('C', 'Y'))]\n",
      "Right Join Result: [(2, ('B', 'X')), (4, (None, 'Z')), (3, ('C', 'Y'))]\n",
      "Full Join Result: [(2, ('B', 'X')), (4, (None, 'Z')), (1, ('A', None)), (3, ('C', 'Y'))]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"JoinExample\")\n",
    "\n",
    "# Sample data for RDD1\n",
    "rdd1 = sc.parallelize([(1, 'A'), (2, 'B'), (3, 'C')])\n",
    "\n",
    "# Sample data for RDD2\n",
    "rdd2 = sc.parallelize([(2, 'X'), (3, 'Y'), (4, 'Z')])\n",
    "\n",
    "# Perform left join\n",
    "left_join_rdd = rdd1.leftOuterJoin(rdd2)\n",
    "print(\"Left Join Result:\", left_join_rdd.collect())\n",
    "\n",
    "# Perform right join\n",
    "right_join_rdd = rdd1.rightOuterJoin(rdd2)\n",
    "print(\"Right Join Result:\", right_join_rdd.collect())\n",
    "\n",
    "# Perform full join\n",
    "full_join_rdd = rdd1.fullOuterJoin(rdd2)\n",
    "print(\"Full Join Result:\", full_join_rdd.collect())\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793387d3-39b6-41c8-b9d9-c2f3688c0ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartesian function A X B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65d1839f-1c27-47cc-a24a-19b5662a3060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 1), ('A', 2), ('A', 3), ('B', 1), ('B', 2), ('B', 3), ('C', 1), ('C', 2), ('C', 3)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"CartesianExample\")\n",
    "\n",
    "# Sample data for RDD1\n",
    "rdd1 = sc.parallelize(['A', 'B', 'C'])\n",
    "\n",
    "# Sample data for RDD2\n",
    "rdd2 = sc.parallelize([1, 2, 3])\n",
    "\n",
    "# Compute the Cartesian product of the two RDDs\n",
    "cartesian_rdd = rdd1.cartesian(rdd2)\n",
    "\n",
    "# Print the result\n",
    "print(cartesian_rdd.collect())\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ec85a9a-8d71-4af8-9ce3-3e24f25b763d",
   "metadata": {},
   "source": [
    "Reduce function using RDD\n",
    "\n",
    "We define a function add() that takes two arguments and returns their sum.\n",
    "We use the reduce() function to sum all the elements of the RDD by applying the add() function. The reduce() function iteratively applies the binary operator (in this case, addition) to pairs of elements in the RDD until it produces a single result.\n",
    "The result of the reduction is the total sum of all elements in the RDD.\n",
    "Finally, we stop the SparkContext to release the resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a912a6-d996-47e5-8ad6-d933a84a5f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum: 15\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"ReduceExample\")\n",
    "\n",
    "# Sample data\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Create an RDD from the data using parallelize\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Define a function to add two numbers\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "# Use reduce() to sum all the elements of the RDD\n",
    "total_sum = rdd.reduce(add)\n",
    "\n",
    "# Print the result\n",
    "print(\"Total Sum:\", total_sum)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35529c19-e805-4623-8172-a1defc7c6f7c",
   "metadata": {},
   "source": [
    "Serialization and deserialization are processes used to convert data structures or objects into a format that can be easily transmitted over a network or stored in a file, and then reconstructing the original data structure or object from that format. This is commonly used in distributed systems like Apache Spark for transferring data between nodes or persisting data to disk.\n",
    "\n",
    "Serialization:\n",
    "Serialization is the process of converting a data structure or object into a sequence of bytes or a string representation that can be easily transmitted over a network or stored in a file. During serialization, the data structure or object is flattened into a format that can be reconstructed later.\n",
    "\n",
    "Common serialization formats include:\n",
    "\n",
    "JSON (JavaScript Object Notation): A lightweight data interchange format that is easy for humans to read and write and easy for machines to parse and generate.\n",
    "XML (eXtensible Markup Language): A markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable.\n",
    "Protocol Buffers (protobuf): A method developed by Google for serializing structured data. It is designed to be more efficient than JSON or XML, especially for large datasets.\n",
    "Apache Avro: A data serialization system that provides rich data structures, compact binary data format, and a schema that can be used to define data types.\n",
    "Deserialization:\n",
    "Deserialization is the process of reconstructing a data structure or object from its serialized form. It involves parsing the serialized data and rebuilding the original data structure or object.\n",
    "\n",
    "For example, if data is serialized in JSON format, the deserialization process would involve parsing the JSON string and converting it back into the original data structure or object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "742fe0d5-5390-487d-bf69-f8b937fc6f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized JSON string: {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\n",
      "Deserialized Data: {'name': 'John', 'age': 30, 'city': 'New York'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Sample data structure\n",
    "data = {\n",
    "    \"name\": \"John\",\n",
    "    \"age\": 30,\n",
    "    \"city\": \"New York\"\n",
    "}\n",
    "\n",
    "# Serialization to JSON string\n",
    "json_string = json.dumps(data)\n",
    "\n",
    "# json.dumps() function is used to serialize the Python dictionary data into a JSON string\n",
    "print(\"Serialized JSON string:\", json_string)\n",
    "\n",
    "# Deserialization from JSON string\n",
    "parsed_data = json.loads(json_string)\n",
    "# json.loads() is used to deserialize the JSON string back into a Python dictionary.\n",
    "print(\"Deserialized Data:\", parsed_data)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "451404c5-2492-4efd-ad6b-3717ddd32c72",
   "metadata": {},
   "source": [
    "In PySpark, the reduceByKey() function is used to aggregate values of the same key in an RDD (Resilient Distributed Dataset). It's a transformation operation that is particularly useful when you want to perform aggregation operations, such as summing up values for each key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "586c3901-ca37-4d59-a37b-8a3692b14d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 4\n",
      "b: 6\n",
      "c: 5\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"ReduceByKeyExample\")\n",
    "\n",
    "# Create an RDD with key-value pairs\n",
    "data = [(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4), (\"c\", 5)]\n",
    "\n",
    "# Parallelize the data into an RDD\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Apply reduceByKey() to aggregate values for each key\n",
    "result = rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Collect the result\n",
    "output = result.collect()\n",
    "\n",
    "# Print the result\n",
    "for key, value in output:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93c75384-06d0-4694-be14-30429ef3c366",
   "metadata": {},
   "source": [
    "group by key vs reduce by key :\n",
    "reduceByKey(): This function reduces data by key. It performs the reduction locally on each partition and then merges the results together. It's more efficient than groupByKey() because it performs the aggregation on each partition before shuffling the data across partitions. It's preferable when the aggregation function is both associative and commutative.\n",
    "groupByKey(): This function groups the values for each key in the RDD. It doesn't perform any aggregation but simply groups the values into an iterator. It can be less efficient, especially on large datasets, because it shuffles all the data across partitions without any local aggregation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da9168a7-5442-44ef-b93e-6a46c77f138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/09 22:07:57 WARN Utils: Your hostname, Vikashs-Laptop.local resolves to a loopback address: 127.0.0.1; using 192.168.1.5 instead (on interface en0)\n",
      "24/05/09 22:07:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/09 22:07:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [1, 3]\n",
      "b: [2, 4]\n",
      "c: [5]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"GroupByKeyExample\")\n",
    "\n",
    "# Create an RDD with key-value pairs\n",
    "data = [(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4), (\"c\", 5)]\n",
    "\n",
    "# Parallelize the data into an RDD\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Apply groupByKey() to group values for each key\n",
    "result = rdd.groupByKey().mapValues(list)\n",
    "\n",
    "# Collect the result\n",
    "output = result.collect()\n",
    "\n",
    "# Print the result\n",
    "for key, value in output:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a451daf-c0d0-4a40-a66e-253382dee46e",
   "metadata": {},
   "source": [
    "aggreatedkey(accumulator,seq_op,comb_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34f435ec-5ff3-44fd-9e10-95dd36c7697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate Sales:\n",
      "('product1', (250.0, 2))\n",
      "('product2', (750.0, 3))\n",
      "Average Sales:\n",
      "('product1', 125.0)\n",
      "('product2', 250.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"AggregateByKeyExample\")\n",
    "\n",
    "# Create an RDD with key-value pairs (product, sales_amount)\n",
    "salesData = sc.parallelize([\n",
    "    (\"product1\", 100),\n",
    "    (\"product1\", 150),\n",
    "    (\"product2\", 200),\n",
    "    (\"product2\", 250),\n",
    "    (\"product2\", 300)\n",
    "])\n",
    "\n",
    "# Define the initial zero value and aggregation functions\n",
    "initialSalesCount = (0.0, 0)  # (total_sales, count)\n",
    "seq_op = lambda salesCount, saleAmount: (salesCount[0] + saleAmount, salesCount[1] + 1)\n",
    "comb_op = lambda salesCount1, salesCount2: (salesCount1[0] + salesCount2[0], salesCount1[1] + salesCount2[1])\n",
    "\n",
    "# Apply aggregateByKey() to aggregate values for each key\n",
    "aggregateSales = salesData.aggregateByKey(initialSalesCount, seq_op, comb_op)\n",
    "\n",
    "# Calculate average sales\n",
    "averageSales = aggregateSales.mapValues(lambda salesCount: salesCount[0] / salesCount[1])\n",
    "\n",
    "# Collect and print the result\n",
    "print(\"Aggregate Sales:\")\n",
    "for item in aggregateSales.collect():\n",
    "    print(item)\n",
    "\n",
    "print(\"Average Sales:\")\n",
    "for item in averageSales.collect():\n",
    "    print(item)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58c71ba8-6b2a-4e46-a42d-c0067cbf4480",
   "metadata": {},
   "source": [
    "In PySpark, the countByKey() action is used to count the occurrences of each unique key in an RDD of key-value pairs.\n",
    "It returns a dictionary where each key is a unique key from the RDD,\n",
    "and the corresponding value is the count of occurrences of that key. Here's an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ddeea47-3dc8-4ba0-bfc8-47be0f0bf8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of occurrences of each key:\n",
      "a: 2\n",
      "b: 2\n",
      "c: 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"countByKeyExample\")\n",
    "\n",
    "# Create an RDD with key-value pairs\n",
    "data = [(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4), (\"c\", 5)]\n",
    "\n",
    "# Parallelize the data into an RDD\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Apply countByKey() to count occurrences of each key\n",
    "result = rdd.countByKey()\n",
    "\n",
    "# Print the result\n",
    "print(\"Count of occurrences of each key:\")\n",
    "for key, count in result.items():\n",
    "    print(f\"{key}: {count}\")\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66824177-4094-48f6-a6da-87c37ded8a14",
   "metadata": {},
   "source": [
    "sortByKey(ascending: Optional[bool] = True, numPartitions: Optional[int] = None, keyfunc: Callable[[Any], Any] = <function RDD.<lambda>>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "683e773d-fe59-40eb-8e8c-42d02e93f4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Student Scores RDD by Name:\n",
      "Alice: 85\n",
      "Bob: 90\n",
      "Charlie: 75\n",
      "David: 80\n",
      "Eva: 95\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Define a function to create an RDD with student scores\n",
    "def create_student_scores_rdd(sc):\n",
    "    data = [(\"Alice\", 85), (\"Bob\", 90), (\"Charlie\", 75),(\"Eva\", 95), (\"David\", 80)]\n",
    "    rdd = sc.parallelize(data)\n",
    "    return rdd\n",
    "\n",
    "# Define a function to sort an RDD by key\n",
    "def sort_rdd_by_key(rdd):\n",
    "    sorted_rdd = rdd.sortByKey()\n",
    "    return sorted_rdd\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"SortByKeyExample\")\n",
    "\n",
    "# Create an RDD with student scores\n",
    "student_scores_rdd = create_student_scores_rdd(sc)\n",
    "\n",
    "# Sort the RDD by student names\n",
    "sorted_student_scores_rdd = sort_rdd_by_key(student_scores_rdd)\n",
    "\n",
    "# Collect and print the sorted result\n",
    "sorted_result = sorted_student_scores_rdd.collect()\n",
    "print(\"Sorted Student Scores RDD by Name:\")\n",
    "for name, score in sorted_result:\n",
    "    print(f\"{name}: {score}\")\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "579038d4-bd32-4833-b0d2-4f21c3117233",
   "metadata": {},
   "source": [
    "dataframe to rdd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ea440f-98f0-443b-9923-035bda412cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 00:01:54 WARN Utils: Your hostname, Vikashs-Laptop.local resolves to a loopback address: 127.0.0.1; using 192.168.1.2 instead (on interface en0)\n",
      "24/05/11 00:01:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/11 00:01:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Elements:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Name='Alice', Age=25)\n",
      "Row(Name='Bob', Age=30)\n",
      "Row(Name='Charlie', Age=35)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrameToRDDExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame (replace this with your actual DataFrame creation)\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Convert DataFrame to RDD\n",
    "rdd = df.rdd\n",
    "\n",
    "# Print RDD elements\n",
    "print(\"RDD Elements:\")\n",
    "for row in rdd.collect():\n",
    "    print(row)\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e71731d2-e56f-40fc-ae24-156423e95d8e",
   "metadata": {},
   "source": [
    "glom uses :glom is particularly useful when you need to perform operations that involve aggregating or processing the data within each partition as a whole. It allows you to work with the entire partition's data in a single step, rather than processing each element individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a422b02f-7dc7-48a0-a8fa-b8ffb0a7dab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result after applying glom:\n",
      "[1, 2]\n",
      "[3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"GlomExample\")\n",
    "\n",
    "# Create an RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5], 2)  # Parallelize data into 2 partitions\n",
    "\n",
    "# Apply glom transformation\n",
    "glommed_rdd = rdd.glom()\n",
    "\n",
    "# Collect and print the result\n",
    "result = glommed_rdd.collect()\n",
    "print(\"Result after applying glom:\")\n",
    "for partition in result:\n",
    "    print(partition)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca66a0f3-4ed2-4d7c-9902-0836d32cf2c8",
   "metadata": {},
   "source": [
    "create a rdd with two partition and show the data in each partition using glom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f6bf8b8-fda3-40bb-af70-db18d098ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Elements of RDD: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"PartitionedRDDExample\")\n",
    "\n",
    "# Sample data\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Create an RDD with two partitions\n",
    "rdd = sc.parallelize(data, numSlices=2)\n",
    "\n",
    "# Check the number of partitions\n",
    "print(\"Number of partitions:\", rdd.getNumPartitions())\n",
    "\n",
    "# Display the elements of the RDD\n",
    "print(\"Elements of RDD:\", rdd.collect())\n",
    "\n",
    "glommed_rdd = rdd.glom()\n",
    "\n",
    "# Collect and print the result\n",
    "result = glommed_rdd.collect()\n",
    "print(result)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9aab3-355d-4aa2-9102-180541b5ab4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
