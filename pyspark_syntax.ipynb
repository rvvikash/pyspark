{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a035d4-7a44-41ee-aee5-08175cf4440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/31 20:26:55 WARN Utils: Your hostname, Vikashs-Laptop.local resolves to a loopback address: 127.0.0.1; using 192.168.1.4 instead (on interface en0)\n",
      "25/05/31 20:26:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/31 20:26:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Fixed lists for consistency\n",
    "products = [\"Laptop\", \"Smartphone\", \"Tablet\", \"Monitor\", \"Keyboard\"]\n",
    "regions = [\"North\", \"South\", \"East\", \"West\", \"Central\"]\n",
    "customer_names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\", \"Frank\", \"Grace\", \"Helen\", \"Ian\", \"Jack\"]\n",
    "\n",
    "# sales_data_1\n",
    "sales_data_1 = pd.DataFrame({\n",
    "    \"sale_id\": list(range(1, 101)),\n",
    "    \"customer_name\": [customer_names[i % 10] if i % 10 != 0 else None for i in range(1, 101)],\n",
    "    \"product\": [products[i % 5] if i % 7 != 0 else \"\" for i in range(1, 101)],\n",
    "    \"quantity\": np.random.randint(1, 10, size=100),\n",
    "    \"price\": np.round(np.random.uniform(10.0, 500.0, size=100), 2),\n",
    "    \"sale_date\": pd.date_range(start=\"2023-01-01\", periods=100, freq='D'),\n",
    "    \"region\": [regions[i % 5] if i % 15 != 0 else None for i in range(1, 101)]\n",
    "})\n",
    "\n",
    "# sales_data_2\n",
    "sales_data_2 = pd.DataFrame({\n",
    "    \"sale_id\": list(range(50, 150)),\n",
    "    \"customer_name\": [customer_names[i % 10] if i % 8 != 0 else None for i in range(50, 150)],\n",
    "    \"product\": [products[i % 5] if i % 6 != 0 else \"\" for i in range(50, 150)],\n",
    "    \"quantity\": np.random.randint(1, 15, size=100),\n",
    "    \"price\": np.round(np.random.uniform(5.0, 1000.0, size=100), 2),\n",
    "    \"sale_date\": pd.date_range(start=\"2023-02-15\", periods=100, freq='D'),\n",
    "    \"region\": [regions[i % 5] if i % 12 != 0 else None for i in range(50, 150)],\n",
    "    \"discount\": [round(np.random.uniform(0, 50), 2) if i % 5 != 0 else None for i in range(50, 150)]\n",
    "})\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SalesData\").getOrCreate()\n",
    "\n",
    "schema1 = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"sale_date\", DateType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema2 = StructType(schema1.fields + [StructField(\"discount\", DoubleType(), True)])\n",
    "\n",
    "sales_df1 = spark.createDataFrame(sales_data_1, schema=schema1)\n",
    "sales_df2 = spark.createDataFrame(sales_data_2, schema=schema2)\n",
    "\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea706ceb-e649-4884-a239-c096d2d531c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+--------+------+----------+------+\n",
      "|sale_id|customer_name|   product|quantity| price| sale_date|region|\n",
      "+-------+-------------+----------+--------+------+----------+------+\n",
      "|      1|          Bob|Smartphone|       4|356.08|2023-01-01| South|\n",
      "|      3|        David|   Monitor|       1|304.38|2023-01-03|  West|\n",
      "|      6|        Grace|Smartphone|       5|257.49|2023-01-06| South|\n",
      "|      8|          Ian|   Monitor|       7|316.01|2023-01-08|  West|\n",
      "|     11|          Bob|Smartphone|       6| 16.65|2023-01-11| South|\n",
      "|     13|        David|   Monitor|       5|397.58|2023-01-13|  West|\n",
      "|     16|        Grace|Smartphone|       9|458.07|2023-01-16| South|\n",
      "|     18|          Ian|   Monitor|       2|307.82|2023-01-18|  West|\n",
      "|     21|          Bob|          |       3|315.19|2023-01-21| South|\n",
      "|     23|        David|   Monitor|       6|100.85|2023-01-23|  West|\n",
      "|     26|        Grace|Smartphone|       1|382.07|2023-01-26| South|\n",
      "|     28|          Ian|          |       7| 284.7|2023-01-28|  West|\n",
      "|     31|          Bob|Smartphone|       7|287.69|2023-01-31| South|\n",
      "|     33|        David|   Monitor|       4|258.94|2023-02-02|  West|\n",
      "|     36|        Grace|Smartphone|       5|233.35|2023-02-05| South|\n",
      "|     38|          Ian|   Monitor|       1| 64.38|2023-02-07|  West|\n",
      "|     41|          Bob|Smartphone|       2|121.41|2023-02-10| South|\n",
      "|     43|        David|   Monitor|       1|281.43|2023-02-12|  West|\n",
      "|     46|        Grace|Smartphone|       9|390.53|2023-02-15| South|\n",
      "|     48|          Ian|   Monitor|       7|499.62|2023-02-17|  West|\n",
      "+-------+-------------+----------+--------+------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select column\n",
    "# column_filter=sales_df1.select('product','customer_name',sales_df1.quantity,(sales_df1.quantity+10).alias(\"new_quantity_of_product\")\n",
    "#                               ,lower(sales_df1.region).alias(\"lower_region\"),sales_df1.region,(sales_df1.price)).show()\n",
    "\n",
    "# group by aggregation\n",
    "# groupby_df = sales_df1.groupBy(\"product\").agg(\n",
    "#     round(sum(\"price\"),2).alias(\"total_sales\"),\n",
    "#     sum(\"quantity\").alias(\"total_quantity\"),\n",
    "#     round(avg(\"price\"),2).alias(\"avg_price\")\n",
    "# ).show()\n",
    "\n",
    "# withColumn(colName, col)\n",
    "# sales_df1.withColumn('order_month',substring(sales_df1.sale_date,1,10)).show()\n",
    "# df = sales_df1.withColumn(\"lower_region\", lower(col(\"region\"))).show()\n",
    "\n",
    "# withColumnRenamed(existingCol, newCol)\n",
    "# sales_df1.withColumnRenamed('sale_id','order_id').show()\n",
    "\n",
    "# drop(*cols)To Drop a specific column.\n",
    "# drop=sales_df1.drop('column_name1','column2').show()\n",
    "\n",
    "# dropDuplicates(subset=None)\n",
    "# sales_df1.dropDuplicates().show()\n",
    "# sales_df1.dropDuplicates((\"name1\",\"score1\",\"score2\")).show()\n",
    "\n",
    "# filter(condition): (Its alias ‘where’)\n",
    "# Filter rows using a given condition.\n",
    "# use '&' for 'and‘. '|' for 'or‘. (boolean expressions)\n",
    "# Use column function isin() for multiple search.\n",
    "# Or use IN Operator for SQL Style syntax.\n",
    "\n",
    "# sales_df1.where((sales_df1.sale_id > 12) & (sales_df1.sale_id < 20)).show()\n",
    "# sales_df1.where(sales_df1.region.isin('West','South',null)).show()\n",
    "\n",
    "# sales_df1.where(\n",
    "#     (col(\"region\").isin(\"West\", \"South\")) | (col(\"region\").isNull())\n",
    "# ).show()\n",
    "\n",
    "\n",
    "sales_df1.where(\n",
    "    (col(\"region\").isin(\"West\", \"South\")) & (col(\"region\").isNotNull())\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e0fdcef-dbaa-4f29-8152-7b451f6c4a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+------+------+------+------+\n",
      "|   product|  null|Central|  East| North| South|  West|\n",
      "+----------+------+-------+------+------+------+------+\n",
      "|    Laptop|189.66|    0.0|   0.0|200.55|   0.0|   0.0|\n",
      "|    Tablet|   0.0|    0.0|228.28|   0.0|   0.0|   0.0|\n",
      "|          |   0.0| 424.14|197.19|268.67|308.83|330.22|\n",
      "|  Keyboard|   0.0| 271.26|   0.0|   0.0|   0.0|   0.0|\n",
      "|Smartphone|   0.0|    0.0|   0.0|   0.0|280.97|   0.0|\n",
      "|   Monitor|   0.0|    0.0|   0.0|   0.0|   0.0|271.34|\n",
      "+----------+------+-------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df = sales_df1.groupBy(\"product\").pivot(\"region\").agg(round(avg(\"price\"),2))\n",
    "pivot_df = pivot_df.fillna(0)\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b365787f-06e2-4ea1-a479-ca55e3eb9ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------+\n",
      "|   product|region|avg_price|\n",
      "+----------+------+---------+\n",
      "|    Laptop|  East|      0.0|\n",
      "|    Tablet|  East|   228.28|\n",
      "|          |  East|   197.19|\n",
      "|  Keyboard|  East|      0.0|\n",
      "|Smartphone|  East|      0.0|\n",
      "|   Monitor|  East|      0.0|\n",
      "|    Laptop| North|   200.55|\n",
      "|    Tablet| North|      0.0|\n",
      "|          | North|   268.67|\n",
      "|  Keyboard| North|      0.0|\n",
      "|Smartphone| North|      0.0|\n",
      "|   Monitor| North|      0.0|\n",
      "|    Laptop| South|      0.0|\n",
      "|    Tablet| South|      0.0|\n",
      "|          | South|   308.83|\n",
      "|  Keyboard| South|      0.0|\n",
      "|Smartphone| South|   280.97|\n",
      "|   Monitor| South|      0.0|\n",
      "+----------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpivot\n",
    "# .withColumn(\"region\", lit(\"East\"))\n",
    "# This adds a new column called \"region\" with a constant value \"East\" for all rows.\n",
    "\n",
    "# lit(\"East\") means a literal (static) string.\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "unpivot_df = (\n",
    "    pivot_df.select(\"product\", pivot_df[\"East\"].alias(\"avg_price\")).withColumn(\"region\", lit(\"East\"))\n",
    "    .union(\n",
    "        pivot_df.select(\"product\", pivot_df[\"North\"].alias(\"avg_price\")).withColumn(\"region\", lit(\"North\"))\n",
    "    )\n",
    "    .union(\n",
    "        pivot_df.select(\"product\", pivot_df[\"South\"].alias(\"avg_price\")).withColumn(\"region\", lit(\"South\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "unpivot_df.select(\"product\", \"region\", \"avg_price\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0d910e0-ca2e-4383-8696-272496a530c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+------------------+------------------+------------------+\n",
      "| region|avg_smartphone_price|avg_monitor_price|avg_keyboard_price|  avg_tablet_price|  avg_laptop_price|\n",
      "+-------+--------------------+-----------------+------------------+------------------+------------------+\n",
      "|  South|  280.96941176470585|             NULL|              NULL|              NULL|              NULL|\n",
      "|Central|                NULL|             NULL| 271.2564705882353|              NULL|              NULL|\n",
      "|   East|                NULL|             NULL|              NULL|228.28352941176468|              NULL|\n",
      "|   West|                NULL|271.3370588235294|              NULL|              NULL|              NULL|\n",
      "|  North|                NULL|             NULL|              NULL|              NULL|200.54666666666665|\n",
      "|   NULL|                NULL|             NULL|              NULL|              NULL|189.65666666666667|\n",
      "+-------+--------------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, avg\n",
    "\n",
    "result_df = sales_df1.groupBy(\"region\").agg(\n",
    "    avg(when(col(\"product\") == \"Smartphone\", col(\"price\"))).alias(\"avg_smartphone_price\"),\n",
    "    avg(when(col(\"product\") == \"Monitor\", col(\"price\"))).alias(\"avg_monitor_price\"),\n",
    "    avg(when(col(\"product\") == \"Keyboard\", col(\"price\"))).alias(\"avg_keyboard_price\"),\n",
    "    avg(when(col(\"product\") == \"Tablet\", col(\"price\"))).alias(\"avg_tablet_price\"),\n",
    "    avg(when(col(\"product\") == \"Laptop\", col(\"price\"))).alias(\"avg_laptop_price\")\n",
    ")\n",
    "\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "646802f5-820a-4cb1-885f-bd35994b37aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+------------------+------------------+------------------+\n",
      "| region|avg_smartphone_price|avg_monitor_price|avg_keyboard_price|  avg_tablet_price|  avg_laptop_price|\n",
      "+-------+--------------------+-----------------+------------------+------------------+------------------+\n",
      "|  South|              238.82|             NULL|              NULL|              NULL|              NULL|\n",
      "|Central|                 0.0|             NULL| 271.2564705882353|              NULL|              NULL|\n",
      "|   East|                 0.0|             NULL|              NULL|228.28352941176468|              NULL|\n",
      "|   West|                 0.0|271.3370588235294|              NULL|              NULL|              NULL|\n",
      "|  North|                 0.0|             NULL|              NULL|              NULL|200.54666666666665|\n",
      "|   NULL|                 0.0|             NULL|              NULL|              NULL|189.65666666666667|\n",
      "+-------+--------------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, avg\n",
    "\n",
    "result_df = sales_df1.groupBy(\"region\").agg(\n",
    "    round(avg(when(sales_df1.product == \"Smartphone\", sales_df1.price).otherwise(0)), 2).alias(\"avg_smartphone_price\"),\n",
    "    avg(when(sales_df1.product == \"Monitor\", sales_df1.price)).alias(\"avg_monitor_price\"),\n",
    "    avg(when(sales_df1.product == \"Keyboard\", sales_df1.price)).alias(\"avg_keyboard_price\"),\n",
    "    avg(when(sales_df1.product == \"Tablet\", sales_df1.price)).alias(\"avg_tablet_price\"),\n",
    "    avg(when(sales_df1.product == \"Laptop\", sales_df1.price)).alias(\"avg_laptop_price\")\n",
    ")\n",
    "\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc28701e-88bd-44fd-9f08-d7d385d948eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+------+----------+------------------+\n",
      "|sale_id| region| sale_date| price|price_rank| running_sum_price|\n",
      "+-------+-------+----------+------+----------+------------------+\n",
      "|     90|   NULL|2023-03-31|214.98|         3|           1137.94|\n",
      "|     39|Central|2023-02-08| 457.2|         3|           2416.79|\n",
      "|     37|   East|2023-02-06|420.56|         3|1901.4700000000003|\n",
      "|     50|  North|2023-02-19|342.24|         3|1617.0900000000001|\n",
      "|     86|  South|2023-03-27| 397.9|         3| 5229.909999999999|\n",
      "|     58|   West|2023-02-27|490.57|         3|           3358.85|\n",
      "+-------+-------+----------+------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum, col, row_number\n",
    "\n",
    "# Window spec for running sum ordered by sale_date\n",
    "window_sum = Window.partitionBy(\"region\").orderBy(\"sale_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Window spec for ranking sales by price (descending) per region\n",
    "window_rank = Window.partitionBy(\"region\").orderBy(col(\"price\").desc())\n",
    "\n",
    "# Calculate running sum of price\n",
    "running_sum_df = sales_df1.withColumn(\"running_sum_price\", sum(col(\"price\")).over(window_sum))\n",
    "\n",
    "# Add row number to rank prices per region descending\n",
    "ranked_df = running_sum_df.withColumn(\"price_rank\", row_number().over(window_rank))\n",
    "\n",
    "# Filter or get the 3rd highest sale price per region\n",
    "third_highest_df = ranked_df.filter(col(\"price_rank\") == 3)\n",
    "\n",
    "# Show the 3rd highest sale in each region\n",
    "third_highest_df.select(\"sale_id\", \"region\", \"sale_date\", \"price\", \"price_rank\", \"running_sum_price\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0b43992-4493-46be-a02e-076bd2300807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+------+----------+------------------+\n",
      "|sale_id| region| sale_date| price|price_rank| running_sum_price|\n",
      "+-------+-------+----------+------+----------+------------------+\n",
      "|     90|   NULL|2023-03-31|214.98|         3|           1137.94|\n",
      "|     39|Central|2023-02-08| 457.2|         3|           2416.79|\n",
      "|     37|   East|2023-02-06|420.56|         3|1901.4700000000003|\n",
      "|     50|  North|2023-02-19|342.24|         3|1617.0900000000001|\n",
      "|     86|  South|2023-03-27| 397.9|         3| 5229.909999999999|\n",
      "|     58|   West|2023-02-27|490.57|         3|           3358.85|\n",
      "+-------+-------+----------+------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum, row_number\n",
    "\n",
    "# Window for running sum ordered by sale_date\n",
    "window_sum = Window.partitionBy(\"region\").orderBy(\"sale_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Window for ranking sales by price descending per region\n",
    "window_rank = Window.partitionBy(\"region\").orderBy(sales_df1.price.desc())\n",
    "\n",
    "# Calculate running sum\n",
    "running_sum_df = sales_df1.withColumn(\"running_sum_price\", sum(sales_df1.price).over(window_sum))\n",
    "\n",
    "# Add row number rank by price descending\n",
    "ranked_df = running_sum_df.withColumn(\"price_rank\", row_number().over(window_rank))\n",
    "\n",
    "# Filter 3rd highest sales per region\n",
    "third_highest_df = ranked_df.filter(ranked_df.price_rank == 3)\n",
    "\n",
    "third_highest_df.select(\"sale_id\", \"region\", \"sale_date\", \"price\", \"price_rank\", \"running_sum_price\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23941dbb-84f2-46d2-af3c-ec2ac8b4d564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+------+------------------+-------+\n",
      "|sale_id|customer_name|   product| price|         avg_price| region|\n",
      "+-------+-------------+----------+------+------------------+-------+\n",
      "|      5|        Frank|    Laptop|234.34|196.91666666666663|  North|\n",
      "|     10|         NULL|    Laptop|372.74|196.91666666666663|  North|\n",
      "|      4|          Eva|  Keyboard|423.94| 271.2564705882353|Central|\n",
      "|      1|          Bob|Smartphone|356.08|280.96941176470585|  South|\n",
      "|      3|        David|   Monitor|304.38| 271.3370588235294|   West|\n",
      "|      8|          Ian|   Monitor|316.01| 271.3370588235294|   West|\n",
      "|     15|        Frank|    Laptop|379.67|196.91666666666663|   NULL|\n",
      "|     22|      Charlie|    Tablet|256.35|228.28352941176468|   East|\n",
      "|     14|          Eva|          |453.99| 308.4628571428571|Central|\n",
      "|     21|          Bob|          |315.19| 308.4628571428571|  South|\n",
      "|     24|          Eva|  Keyboard|389.38| 271.2564705882353|Central|\n",
      "|     16|        Grace|Smartphone|458.07|280.96941176470585|  South|\n",
      "|     13|        David|   Monitor|397.58| 271.3370588235294|   West|\n",
      "|     18|          Ian|   Monitor|307.82| 271.3370588235294|   West|\n",
      "|     25|        Frank|    Laptop|213.14|196.91666666666663|  North|\n",
      "|     30|         NULL|    Laptop|270.16|196.91666666666663|   NULL|\n",
      "|     27|        Helen|    Tablet| 427.4|228.28352941176468|   East|\n",
      "|     29|         Jack|  Keyboard|282.21| 271.2564705882353|Central|\n",
      "|     26|        Grace|Smartphone|382.07|280.96941176470585|  South|\n",
      "|     31|          Bob|Smartphone|287.69|280.96941176470585|  South|\n",
      "+-------+-------------+----------+------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "# we find the customer name whose product price higher then the average price of product for that region \n",
    "\n",
    "avg_price_df = sales_df1.groupBy(\"product\").agg(avg(\"price\").alias(\"avg_price\"))\n",
    "\n",
    "result_df = sales_df1.join(avg_price_df, on=\"product\").filter(sales_df1.price > avg_price_df.avg_price)\n",
    "\n",
    "result_df.select(\"sale_id\", \"customer_name\", \"product\", \"price\", \"avg_price\", \"region\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26111ece-8160-4e01-8871-3d100af4f4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+--------+--------+------+----------+-------+-----------+----------+\n",
      "|sale_id|customer_name| product|quantity| price| sale_date| region|first_price|last_price|\n",
      "+-------+-------------+--------+--------+------+----------+-------+-----------+----------+\n",
      "|     15|        Frank|  Laptop|       8|379.67|2023-01-15|   NULL|     379.67|    379.67|\n",
      "|     30|         NULL|  Laptop|       4|270.16|2023-01-30|   NULL|     379.67|    270.16|\n",
      "|     45|        Frank|  Laptop|       6|130.48|2023-02-14|   NULL|     379.67|    130.48|\n",
      "|     60|         NULL|  Laptop|       5|  70.9|2023-03-01|   NULL|     379.67|      70.9|\n",
      "|     75|        Frank|  Laptop|       6| 71.75|2023-03-16|   NULL|     379.67|     71.75|\n",
      "|     90|         NULL|  Laptop|       6|214.98|2023-03-31|   NULL|     379.67|    214.98|\n",
      "|      4|          Eva|Keyboard|       3|423.94|2023-01-04|Central|     423.94|    423.94|\n",
      "|      9|         Jack|Keyboard|       2| 95.92|2023-01-09|Central|     423.94|     95.92|\n",
      "|     14|          Eva|        |       5|453.99|2023-01-14|Central|     423.94|    453.99|\n",
      "|     19|         Jack|Keyboard|       5|253.04|2023-01-19|Central|     423.94|    253.04|\n",
      "|     24|          Eva|Keyboard|       1|389.38|2023-01-24|Central|     423.94|    389.38|\n",
      "|     29|         Jack|Keyboard|       2|282.21|2023-01-29|Central|     423.94|    282.21|\n",
      "|     34|          Eva|Keyboard|       4| 61.11|2023-02-03|Central|     423.94|     61.11|\n",
      "|     39|         Jack|Keyboard|       7| 457.2|2023-02-08|Central|     423.94|     457.2|\n",
      "|     44|          Eva|Keyboard|       9|367.56|2023-02-13|Central|     423.94|    367.56|\n",
      "|     49|         Jack|        |       5|476.09|2023-02-18|Central|     423.94|    476.09|\n",
      "|     54|          Eva|Keyboard|       8|364.92|2023-02-23|Central|     423.94|    364.92|\n",
      "|     59|         Jack|Keyboard|       9| 253.0|2023-02-28|Central|     423.94|     253.0|\n",
      "|     64|          Eva|Keyboard|       3|175.38|2023-03-05|Central|     423.94|    175.38|\n",
      "|     69|         Jack|Keyboard|       8|125.74|2023-03-10|Central|     423.94|    125.74|\n",
      "+-------+-------------+--------+--------+------+----------+-------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "window_spec = Window.partitionBy(\"region\").orderBy(\"sale_date\")\n",
    "df = sales_df1.withColumn(\"first_price\", first(\"price\").over(window_spec))\n",
    "df = df.withColumn(\"last_price\", last(\"price\").over(window_spec)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f51a6c-b413-4760-9e13-0ffe0546db60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
