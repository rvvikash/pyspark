{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c9cb6a-d960-47d9-b8af-15e9efcea7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/anaconda3/lib/python3.11/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/anaconda3/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2b9a9d5-9776-43b2-a3b9-e3eb1ed615ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n",
      "8\n",
      "<bound method RDD.glom of ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:289>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Create an RDD from a Python list\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Perform operations on the RDD\n",
    "\n",
    "# result = rdd.map(lambda x: x * 2).collect()\n",
    "def square(x):\n",
    "    return x*2\n",
    "    \n",
    "result=rdd.map(square).collect()    \n",
    "\n",
    "print(result)  # Output: [2, 4, 6, 8, 10]\n",
    "print(rdd.getNumPartitions()) \n",
    "# to check the number of partition  for the rdd\n",
    "print(rdd.glom)\n",
    "# it print the record from each partition we need to avoid run this if data is huge in one partition or we can do for print some lines\n",
    "\n",
    "# help(sc.parallelize) \n",
    "# it helps to read about fucntion using help \n",
    "rdd.collect()\n",
    "sc.stop()\n",
    "# using collect function we can check the rdd output using list we used collect as data is not huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56d1847e-665a-4007-bf27-b4600da3a27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Get or create a SparkContext\n",
    "spark_context = SparkContext.getOrCreate()\n",
    "\n",
    "# Access the Spark UI URL\n",
    "ui_url = spark_context.uiWebUrl\n",
    "\n",
    "print(ui_url)\n",
    "spark_context.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5701e734-5dc4-41bb-a221-f87cbd2be074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 34|\n",
      "|    Bob| 45|\n",
      "|Charlie| 29|\n",
      "|  David| 41|\n",
      "|    Eve| 38|\n",
      "+-------+---+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a list of tuples (or a list of lists) representing your data\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29), (\"David\", 41), (\"Eve\", 38)]\n",
    "\n",
    "# Create a DataFrame using the created SparkSession and the list of tuples\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "# To show the schema of data frame \n",
    "df.printSchema()\n",
    "spark.stop() \n",
    "# Do not forget to stop the spark application other wise it will give error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "765cabab-3b38-4cde-bb58-46a033ddef49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, (['A'], ['null'])), (2, (['B'], ['X'])), (3, (['C'], ['Y'])), (4, (['D'], ['null'])), (5, (['null'], ['Z']))]\n"
     ]
    }
   ],
   "source": [
    "# Import SparkContext from the pyspark module\n",
    "from pyspark import SparkContext\n",
    "\n",
    "if 'sc' in globals():\n",
    "    sc.stop()\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext()\n",
    "\n",
    "# Sample data for RDD1\n",
    "rdd1_data = sc.parallelize([(1, 'A'), (2, 'B'), (3, 'C'), (4, 'D')])\n",
    "\n",
    "# Sample data for RDD2\n",
    "rdd2_data = sc.parallelize([(2, 'X'), (3, 'Y'), (5, 'Z')])\n",
    "\n",
    "\n",
    "# Convert RDD1 and RDD2 to key-value pair RDDs\n",
    "rdd1_kv = rdd1_data.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "rdd2_kv = rdd2_data.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "# Perform left join using cogroup()\n",
    "joined_rdd = rdd1_kv.cogroup(rdd2_kv)\n",
    "\n",
    "# Process the joined RDD to get the desired result\n",
    "def process_join(kv):\n",
    "    key = kv[0]\n",
    "    values1 = list(kv[1][0]) if kv[1][0] else ['null']\n",
    "    values2 = list(kv[1][1]) if kv[1][1] else ['null']\n",
    "    return (key, (values1, values2))\n",
    "\n",
    "result = joined_rdd.map(process_join)\n",
    "\n",
    "# Collect the result as a list and print it\n",
    "print(result.collect())\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3b72d5b-adc8-49b5-8126-994b16257dd1",
   "metadata": {},
   "source": [
    "ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by getOrCreate at /var/folders/ht/5d19w4p161dgmlf2yzfbz7qh0000gn/T/ipykernel_88175/2416547275.py:4 \n",
    "To deal with this error we should need to stop the other spark context or session so in this case we need to use\n",
    "if 'sc' in globals():\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42d1ae67-3f09-453c-a82b-e9bfbd4a7ad4",
   "metadata": {},
   "source": [
    "join in rdd using join keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec3646bb-cc53-4b14-8001-451cbfc93269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, ('B', 'X')), (3, ('C', 'Y'))]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"JoinExample\")\n",
    "\n",
    "# Sample data for RDD1\n",
    "rdd1 = sc.parallelize([(1, 'A'), (2, 'B'), (3, 'C')])\n",
    "\n",
    "# Sample data for RDD2\n",
    "rdd2 = sc.parallelize([(2, 'X'), (3, 'Y'), (4, 'Z')])\n",
    "\n",
    "# Perform inner join on the two RDDs\n",
    "joined_rdd = rdd1.join(rdd2)\n",
    "\n",
    "# Print the result\n",
    "print(joined_rdd.collect())\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a9cbcf0-076e-4f24-a993-f0d5a78f3700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Join Result: [(2, ('B', 'X')), (1, ('A', None)), (3, ('C', 'Y'))]\n",
      "Right Join Result: [(2, ('B', 'X')), (4, (None, 'Z')), (3, ('C', 'Y'))]\n",
      "Full Join Result: [(2, ('B', 'X')), (4, (None, 'Z')), (1, ('A', None)), (3, ('C', 'Y'))]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"JoinExample\")\n",
    "\n",
    "# Sample data for RDD1\n",
    "rdd1 = sc.parallelize([(1, 'A'), (2, 'B'), (3, 'C')])\n",
    "\n",
    "# Sample data for RDD2\n",
    "rdd2 = sc.parallelize([(2, 'X'), (3, 'Y'), (4, 'Z')])\n",
    "\n",
    "# Perform left join\n",
    "left_join_rdd = rdd1.leftOuterJoin(rdd2)\n",
    "print(\"Left Join Result:\", left_join_rdd.collect())\n",
    "\n",
    "# Perform right join\n",
    "right_join_rdd = rdd1.rightOuterJoin(rdd2)\n",
    "print(\"Right Join Result:\", right_join_rdd.collect())\n",
    "\n",
    "# Perform full join\n",
    "full_join_rdd = rdd1.fullOuterJoin(rdd2)\n",
    "print(\"Full Join Result:\", full_join_rdd.collect())\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793387d3-39b6-41c8-b9d9-c2f3688c0ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartesian function A X B this is very we should have to never use this one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65d1839f-1c27-47cc-a24a-19b5662a3060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 1), ('A', 2), ('A', 3), ('B', 1), ('B', 2), ('B', 3), ('C', 1), ('C', 2), ('C', 3)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"CartesianExample\")\n",
    "\n",
    "# Sample data for RDD1\n",
    "rdd1 = sc.parallelize(['A', 'B', 'C'])\n",
    "\n",
    "# Sample data for RDD2\n",
    "rdd2 = sc.parallelize([1, 2, 3])\n",
    "\n",
    "# Compute the Cartesian product of the two RDDs\n",
    "cartesian_rdd = rdd1.cartesian(rdd2)\n",
    "\n",
    "# Print the result\n",
    "print(cartesian_rdd.collect())\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ec85a9a-8d71-4af8-9ce3-3e24f25b763d",
   "metadata": {},
   "source": [
    "Reduce function using RDD\n",
    "\n",
    "We define a function add() that takes two arguments and returns their sum.\n",
    "We use the reduce() function to sum all the elements of the RDD by applying the add() function. The reduce() function iteratively applies the binary operator (in this case, addition) to pairs of elements in the RDD until it produces a single result.\n",
    "The result of the reduction is the total sum of all elements in the RDD.\n",
    "Finally, we stop the SparkContext to release the resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82a912a6-d996-47e5-8ad6-d933a84a5f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Total Sum: 15\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"ReduceExample\")\n",
    "\n",
    "# Sample data\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Create an RDD from the data using parallelize\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Define a function to add two numbers\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "# Use reduce() to sum all the elements of the RDD\n",
    "total_sum = rdd.reduce(add)\n",
    "\n",
    "total_sum1=rdd.reduce(lambda x,y:x+y)\n",
    "print(total_sum1)\n",
    "\n",
    "\n",
    "# Print the result\n",
    "print(\"Total Sum:\", total_sum)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35529c19-e805-4623-8172-a1defc7c6f7c",
   "metadata": {},
   "source": [
    "Serialization and deserialization are processes used to convert data structures or objects into a format that can be easily transmitted over a network or stored in a file, and then reconstructing the original data structure or object from that format. This is commonly used in distributed systems like Apache Spark for transferring data between nodes or persisting data to disk.\n",
    "\n",
    "Serialization:\n",
    "Serialization is the process of converting a data structure or object into a sequence of bytes or a string representation that can be easily transmitted over a network or stored in a file. During serialization, the data structure or object is flattened into a format that can be reconstructed later.\n",
    "\n",
    "Common serialization formats include:\n",
    "\n",
    "JSON (JavaScript Object Notation): A lightweight data interchange format that is easy for humans to read and write and easy for machines to parse and generate.\n",
    "XML (eXtensible Markup Language): A markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable.\n",
    "Protocol Buffers (protobuf): A method developed by Google for serializing structured data. It is designed to be more efficient than JSON or XML, especially for large datasets.\n",
    "Apache Avro: A data serialization system that provides rich data structures, compact binary data format, and a schema that can be used to define data types.\n",
    "Deserialization:\n",
    "Deserialization is the process of reconstructing a data structure or object from its serialized form. It involves parsing the serialized data and rebuilding the original data structure or object.\n",
    "\n",
    "For example, if data is serialized in JSON format, the deserialization process would involve parsing the JSON string and converting it back into the original data structure or object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "742fe0d5-5390-487d-bf69-f8b937fc6f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized JSON string: {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\n",
      "<class 'str'>\n",
      "Deserialized Data: {'name': 'John', 'age': 30, 'city': 'New York'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Sample data structure\n",
    "data = {\n",
    "    \"name\": \"John\",\n",
    "    \"age\": 30,\n",
    "    \"city\": \"New York\"\n",
    "}\n",
    "\n",
    "# Serialization to JSON string\n",
    "json_string = json.dumps(data)\n",
    "\n",
    "# json.dumps() function is used to serialize the Python dictionary data into a JSON string\n",
    "print(\"Serialized JSON string:\", json_string)\n",
    "print(type(json_string))\n",
    "\n",
    "# Deserialization from JSON string\n",
    "parsed_data = json.loads(json_string)\n",
    "# json.loads() is used to deserialize the JSON string back into a Python dictionary.\n",
    "print(\"Deserialized Data:\", parsed_data)\n",
    "print(type(parsed_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "451404c5-2492-4efd-ad6b-3717ddd32c72",
   "metadata": {},
   "source": [
    "In PySpark, the reduceByKey() function is used to aggregate values of the same key in an RDD (Resilient Distributed Dataset). It's a transformation operation that is particularly useful when you want to perform aggregation operations, such as summing up values for each key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "586c3901-ca37-4d59-a37b-8a3692b14d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 4\n",
      "b: 6\n",
      "c: 5\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"ReduceByKeyExample\")\n",
    "\n",
    "# Create an RDD with key-value pairs\n",
    "data = [(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4), (\"c\", 5)]\n",
    "\n",
    "# Parallelize the data into an RDD\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Apply reduceByKey() to aggregate values for each key\n",
    "result = rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Collect the result\n",
    "output = result.collect()\n",
    "\n",
    "# Print the result\n",
    "for key, value in output:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93c75384-06d0-4694-be14-30429ef3c366",
   "metadata": {},
   "source": [
    "group by key vs reduce by key :\n",
    "reduceByKey(): This function reduces data by key. It performs the reduction locally on each partition and then merges the results together. It's more efficient than groupByKey() because it performs the aggregation on each partition before shuffling the data across partitions. It's preferable when the aggregation function is both associative and commutative.\n",
    "groupByKey(): This function groups the values for each key in the RDD. It doesn't perform any aggregation but simply groups the values into an iterator. It can be less efficient, especially on large datasets, because it shuffles all the data across partitions without any local aggregation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da9168a7-5442-44ef-b93e-6a46c77f138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [1, 3]\n",
      "b: [2, 4]\n",
      "c: [5]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"GroupByKeyExample\")\n",
    "\n",
    "# Create an RDD with key-value pairs\n",
    "data = [(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4), (\"c\", 5)]\n",
    "\n",
    "# Parallelize the data into an RDD\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Apply groupByKey() to group values for each key\n",
    "result = rdd.groupByKey().mapValues(list)\n",
    "\n",
    "# Collect the result\n",
    "output = result.collect()\n",
    "\n",
    "# Print the result\n",
    "for key, value in output:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a451daf-c0d0-4a40-a66e-253382dee46e",
   "metadata": {},
   "source": [
    "aggreatedkey(accumulator,seq_op,comb_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34f435ec-5ff3-44fd-9e10-95dd36c7697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate Sales:\n",
      "('product1', (250.0, 2))\n",
      "('product2', (750.0, 3))\n",
      "Average Sales:\n",
      "('product1', 125.0)\n",
      "('product2', 250.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"AggregateByKeyExample\")\n",
    "\n",
    "# Create an RDD with key-value pairs (product, sales_amount)\n",
    "salesData = sc.parallelize([\n",
    "    (\"product1\", 100),\n",
    "    (\"product1\", 150),\n",
    "    (\"product2\", 200),\n",
    "    (\"product2\", 250),\n",
    "    (\"product2\", 300)\n",
    "])\n",
    "\n",
    "# Define the initial zero value and aggregation functions\n",
    "initialSalesCount = (0.0, 0)  # (total_sales, count)\n",
    "seq_op = lambda salesCount, saleAmount: (salesCount[0] + saleAmount, salesCount[1] + 1)\n",
    "comb_op = lambda salesCount1, salesCount2: (salesCount1[0] + salesCount2[0], salesCount1[1] + salesCount2[1])\n",
    "\n",
    "# Apply aggregateByKey() to aggregate values for each key\n",
    "aggregateSales = salesData.aggregateByKey(initialSalesCount, seq_op, comb_op)\n",
    "\n",
    "# Calculate average sales\n",
    "averageSales = aggregateSales.mapValues(lambda salesCount: salesCount[0] / salesCount[1])\n",
    "\n",
    "# Collect and print the result\n",
    "print(\"Aggregate Sales:\")\n",
    "for item in aggregateSales.collect():\n",
    "    print(item)\n",
    "\n",
    "print(\"Average Sales:\")\n",
    "for item in averageSales.collect():\n",
    "    print(item)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58c71ba8-6b2a-4e46-a42d-c0067cbf4480",
   "metadata": {},
   "source": [
    "In PySpark, the countByKey() action is used to count the occurrences of each unique key in an RDD of key-value pairs.\n",
    "It returns a dictionary where each key is a unique key from the RDD,\n",
    "and the corresponding value is the count of occurrences of that key. Here's an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ddeea47-3dc8-4ba0-bfc8-47be0f0bf8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of occurrences of each key:\n",
      "a: 2\n",
      "b: 2\n",
      "c: 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"countByKeyExample\")\n",
    "\n",
    "# Create an RDD with key-value pairs\n",
    "data = [(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4), (\"c\", 5)]\n",
    "\n",
    "# Parallelize the data into an RDD\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Apply countByKey() to count occurrences of each key\n",
    "result = rdd.countByKey()\n",
    "\n",
    "# Print the result\n",
    "print(\"Count of occurrences of each key:\")\n",
    "for key, count in result.items():\n",
    "    print(f\"{key}: {count}\")\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66824177-4094-48f6-a6da-87c37ded8a14",
   "metadata": {},
   "source": [
    "sortByKey(ascending: Optional[bool] = True, numPartitions: Optional[int] = None, keyfunc: Callable[[Any], Any] = <function RDD.<lambda>>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "683e773d-fe59-40eb-8e8c-42d02e93f4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Student Scores RDD by Name:\n",
      "Alice: 85\n",
      "Bob: 90\n",
      "Charlie: 75\n",
      "David: 80\n",
      "Eva: 95\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Define a function to create an RDD with student scores\n",
    "def create_student_scores_rdd(sc):\n",
    "    data = [(\"Alice\", 85), (\"Bob\", 90), (\"Charlie\", 75),(\"Eva\", 95), (\"David\", 80)]\n",
    "    rdd = sc.parallelize(data)\n",
    "    return rdd\n",
    "\n",
    "# Define a function to sort an RDD by key\n",
    "def sort_rdd_by_key(rdd):\n",
    "    sorted_rdd = rdd.sortByKey()\n",
    "    return sorted_rdd\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"SortByKeyExample\")\n",
    "\n",
    "# Create an RDD with student scores\n",
    "student_scores_rdd = create_student_scores_rdd(sc)\n",
    "\n",
    "# Sort the RDD by student names\n",
    "sorted_student_scores_rdd = sort_rdd_by_key(student_scores_rdd)\n",
    "\n",
    "# Collect and print the sorted result\n",
    "sorted_result = sorted_student_scores_rdd.collect()\n",
    "print(\"Sorted Student Scores RDD by Name:\")\n",
    "for name, score in sorted_result:\n",
    "    print(f\"{name}: {score}\")\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bf337fe-728e-464d-b6d6-3a83299e5858",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=JSONtoDataFrame, master=local[*]) created by getOrCreate at /var/folders/ht/5d19w4p161dgmlf2yzfbz7qh0000gn/T/ipykernel_17420/1873400294.py:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sorted_rdd\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create a SparkContext\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSortByValueExample\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create an RDD with student scores\u001b[39;00m\n\u001b[1;32m     18\u001b[0m student_scores_rdd \u001b[38;5;241m=\u001b[39m create_student_scores_rdd(sc)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=JSONtoDataFrame, master=local[*]) created by getOrCreate at /var/folders/ht/5d19w4p161dgmlf2yzfbz7qh0000gn/T/ipykernel_17420/1873400294.py:5 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Define a function to create an RDD with student scores\n",
    "def create_student_scores_rdd(sc):\n",
    "    data = [(\"Alice\", 85), (\"Bob\", 90), (\"Charlie\", 75), (\"Eva\", 95), (\"David\", 80)]\n",
    "    rdd = sc.parallelize(data)\n",
    "    return rdd\n",
    "\n",
    "# Define a function to sort an RDD by value\n",
    "def sort_rdd_by_value(rdd):\n",
    "    sorted_rdd = rdd.sortBy(lambda x: x[1])  # Sorting by the second element of each tuple\n",
    "    return sorted_rdd\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"SortByValueExample\")\n",
    "\n",
    "# Create an RDD with student scores\n",
    "student_scores_rdd = create_student_scores_rdd(sc)\n",
    "\n",
    "# Sort the RDD by student scores\n",
    "sorted_student_scores_rdd = sort_rdd_by_value(student_scores_rdd)\n",
    "\n",
    "# Collect and print the sorted result\n",
    "sorted_result = sorted_student_scores_rdd.collect()\n",
    "print(\"Sorted Student Scores RDD by Score:\")\n",
    "for name, score in sorted_result:\n",
    "    print(f\"{name}: {score}\")\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "579038d4-bd32-4833-b0d2-4f21c3117233",
   "metadata": {},
   "source": [
    "dataframe to rdd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ea440f-98f0-443b-9923-035bda412cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 00:01:54 WARN Utils: Your hostname, Vikashs-Laptop.local resolves to a loopback address: 127.0.0.1; using 192.168.1.2 instead (on interface en0)\n",
      "24/05/11 00:01:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/11 00:01:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Elements:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Name='Alice', Age=25)\n",
      "Row(Name='Bob', Age=30)\n",
      "Row(Name='Charlie', Age=35)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrameToRDDExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame (replace this with your actual DataFrame creation)\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Convert DataFrame to RDD\n",
    "rdd = df.rdd\n",
    "\n",
    "# Print RDD elements\n",
    "print(\"RDD Elements:\")\n",
    "for row in rdd.collect():\n",
    "    print(row)\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a296d-8e0b-4380-b2e9-92ba3934bc1a",
   "metadata": {},
   "source": [
    "glom uses :glom is particularly useful when you need to perform operations that involve aggregating or processing the data within each partition as a whole. It allows you to work with the entire partition's data in a single step, rather than processing each element individually."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca66a0f3-4ed2-4d7c-9902-0836d32cf2c8",
   "metadata": {},
   "source": [
    "create a rdd with two partition and show the data in each partition using glom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f6bf8b8-fda3-40bb-af70-db18d098ad05",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=JSONtoDataFrame, master=local[*]) created by getOrCreate at /var/folders/ht/5d19w4p161dgmlf2yzfbz7qh0000gn/T/ipykernel_17420/1873400294.py:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create a SparkContext\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartitionedRDDExample\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Sample data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m10\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=JSONtoDataFrame, master=local[*]) created by getOrCreate at /var/folders/ht/5d19w4p161dgmlf2yzfbz7qh0000gn/T/ipykernel_17420/1873400294.py:5 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"PartitionedRDDExample\")\n",
    "\n",
    "# Sample data\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Create an RDD with two partitions\n",
    "rdd = sc.parallelize(data, numSlices=2)\n",
    "\n",
    "# Check the number of partitions\n",
    "print(\"Number of partitions:\", rdd.getNumPartitions())\n",
    "\n",
    "# Display the elements of the RDD\n",
    "print(\"Elements of RDD:\", rdd.collect())\n",
    "\n",
    "glommed_rdd = rdd.glom()\n",
    "\n",
    "# Collect and print the result\n",
    "result = glommed_rdd.collect()\n",
    "print(result)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fa9aab3-355d-4aa2-9102-180541b5ab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join Result:\n",
      "[(1, ('a', 'x')), (2, ('b', 'y'))]\n",
      "Cogroup Result:\n",
      "[(1, (['a'], ['x'])), (2, (['b'], ['y'])), (3, (['c'], [])), (4, ([], ['z']))]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Create sample RDDs\n",
    "rdd1 = sc.parallelize([(1, \"a\"), (2, \"b\"), (3, \"c\")])\n",
    "rdd2 = sc.parallelize([(1, \"x\"), (2, \"y\"), (4, \"z\")])\n",
    "\n",
    "# Perform join operation\n",
    "joined_rdd = rdd1.join(rdd2)\n",
    "print(\"Join Result:\")\n",
    "print(joined_rdd.collect())\n",
    "# Expected Output: [(1, ('a', 'x')), (2, ('b', 'y'))]\n",
    "\n",
    "# Perform cogroup operation\n",
    "cogrouped_rdd = rdd1.cogroup(rdd2)\n",
    "cogrouped_result = [(k, (list(v1), list(v2))) for k, (v1, v2) in cogrouped_rdd.collect()]\n",
    "print(\"Cogroup Result:\")\n",
    "print(cogrouped_result)\n",
    "# Expected Output: [(1, (['a'], ['x'])), (2, (['b'], ['y'])), (3, (['c'], [])), (4, ([], ['z']))]\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc373645-1c19-401d-9a4d-108e83f42d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem: 1\n",
      "ipsum: 1\n",
      "dolor: 1\n",
      "sit: 2\n",
      "amet,: 1\n",
      "consectetur: 1\n",
      "adipiscing: 1\n",
      "elit: 2\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit elit sit\"\n",
    "\n",
    "# Split the text into words\n",
    "words = text.split()\n",
    "\n",
    "# Initialize an empty dictionary to store word counts\n",
    "word_count = {}\n",
    "\n",
    "# Count the occurrences of each word\n",
    "for item in words:\n",
    "    # If the element is already in the dictionary, increment its count\n",
    "    if item in word_count:\n",
    "        word_count[item] += 1\n",
    "    # If the element is not in the dictionary, add it with count 1\n",
    "    else:\n",
    "        word_count[item] = 1\n",
    "# Print the word counts\n",
    "for word, count in word_count.items():\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b349dd8-2cf7-40c4-827b-81a50dca1ac2",
   "metadata": {},
   "source": [
    "query optimization catalyst in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2869a05-4f32-49c6-b029-1e3c0f6bbdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/27 15:00:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical Plan:\n",
      "== Parsed Logical Plan ==\n",
      "'Project ['name, 'age]\n",
      "+- 'Filter ('age > 21)\n",
      "   +- 'UnresolvedRelation [people], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "name: string, age: bigint\n",
      "Project [name#17, age#18L]\n",
      "+- Filter (age#18L > cast(21 as bigint))\n",
      "   +- SubqueryAlias people\n",
      "      +- View (`people`, [name#17,age#18L,gender#19])\n",
      "         +- LogicalRDD [name#17, age#18L, gender#19], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [name#17, age#18L]\n",
      "+- Filter (isnotnull(age#18L) AND (age#18L > 21))\n",
      "   +- LogicalRDD [name#17, age#18L, gender#19], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [name#17, age#18L]\n",
      "+- *(1) Filter (isnotnull(age#18L) AND (age#18L > 21))\n",
      "   +- *(1) Scan ExistingRDD[name#17,age#18L,gender#19]\n",
      "\n",
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 30|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"CatalystOptimizerExample\").getOrCreate()\n",
    "\n",
    "# Create some sample data\n",
    "data = [(\"Alice\", 25, \"F\"), (\"Bob\", 30, \"M\"), (\"Charlie\", 20, \"M\")]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\", \"gender\"])\n",
    "\n",
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Define a SQL query\n",
    "query = \"\"\"\n",
    "SELECT name, age\n",
    "FROM people\n",
    "WHERE age > 21\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "# Show the logical plan\n",
    "print(\"Logical Plan:\")\n",
    "result_df.explain(extended=True)\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a113947-d7d2-4a53-923d-9ddcd2b0d595",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=JoinExample, master=local) created by __init__ at /var/folders/ht/5d19w4p161dgmlf2yzfbz7qh0000gn/T/ipykernel_4605/3871664899.py:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize SparkSession\u001b[39;00m\n\u001b[1;32m      4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMapTransformationExample\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m----> 5\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create an RDD\u001b[39;00m\n\u001b[1;32m      8\u001b[0m original_rdd \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mparallelize([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=JoinExample, master=local) created by __init__ at /var/folders/ht/5d19w4p161dgmlf2yzfbz7qh0000gn/T/ipykernel_4605/3871664899.py:4 "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"MapTransformationExample\").getOrCreate()\n",
    "sc = SparkContext()\n",
    "\n",
    "# Create an RDD\n",
    "original_rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Apply map transformation to square each element\n",
    "squared_rdd = original_rdd.map(lambda x: x**2)\n",
    "\n",
    "# Output RDD\n",
    "output = squared_rdd.collect()\n",
    "\n",
    "print(output)\n",
    "# Output: [1, 4, 9, 16, 25]\n",
    "sc.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a86bfde-eba7-4fa0-95e9-b693c5ea258c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=MapTransformationExample, master=local[*]) created by getOrCreate at /var/folders/ht/5d19w4p161dgmlf2yzfbz7qh0000gn/T/ipykernel_19560/3350164189.py:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Set up Spark configuration and context\u001b[39;00m\n\u001b[1;32m      5\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD Persistence Example\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create an RDD\u001b[39;00m\n\u001b[1;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m10\u001b[39m], \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=MapTransformationExample, master=local[*]) created by getOrCreate at /var/folders/ht/5d19w4p161dgmlf2yzfbz7qh0000gn/T/ipykernel_19560/3350164189.py:4 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Set up Spark configuration and context\n",
    "conf = SparkConf().setAppName(\"RDD Persistence Example\").setMaster(\"local\")\n",
    "sc = SparkContext()\n",
    "\n",
    "# Create an RDD\n",
    "data = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3)\n",
    "\n",
    "# Persist the RDD in memory\n",
    "data.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# Perform multiple actions on the RDD\n",
    "mean = data.mean()\n",
    "variance = data.variance()\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Variance: {variance}\")\n",
    "\n",
    "# Unpersist the RDD when done\n",
    "data.unpersist()\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8126f0a-4ca9-4458-ad99-fe2dfdc2fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d393ebd-985c-4b2f-befb-f06eb138e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/30 22:11:43 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blank lines: 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Set up Spark configuration and context\n",
    "conf = SparkConf().setAppName(\"Accumulator Example\").setMaster(\"local\")\n",
    "sc = SparkContext()\n",
    "\n",
    "# Create an accumulator\n",
    "blank_lines_accum = sc.accumulator(0)\n",
    "\n",
    "# Create an RDD from a list of lines\n",
    "lines = sc.parallelize([\"Hello World\", \"\", \"This is a line\", \"\", \"Another line\"])\n",
    "\n",
    "# Function to count blank lines\n",
    "def count_blank_lines(line):\n",
    "    if line == \"\":\n",
    "        blank_lines_accum.add(1)\n",
    "    return line\n",
    "\n",
    "# Use the accumulator in a map operation\n",
    "lines.map(count_blank_lines).collect()\n",
    "\n",
    "# Print the number of blank lines\n",
    "print(f\"Number of blank lines: {blank_lines_accum.value}\")\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba83163-3e3d-47d3-830b-68f70f03292a",
   "metadata": {},
   "source": [
    "Setup Spark Context: Initialize the Spark context.\n",
    "Create and Broadcast the Lookup Table: Create a dictionary for the lookup table and broadcast it.\n",
    "Create an RDD of User Records: Create an RDD containing user records with country codes.\n",
    "Use the Broadcast Variable: Use the broadcast variable in a map transformation to add country names to user records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5953f388-2d9e-4f23-86c8-2c7b492f7c42",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Python Spark SQL basic example, master=local[*]) created by getOrCreate at /var/folders/ht/5d19w4p161dgmlf2yzfbz7qh0000gn/T/ipykernel_74229/2060002511.py:7 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Step 1: Initialize Spark Context\u001b[39;00m\n\u001b[1;32m      4\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcast Variable Example\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Step 2: Create and Broadcast the Lookup Table\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     country_lookup \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUS\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnited States\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIN\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndia\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDE\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGermany\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m     }\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Python Spark SQL basic example, master=local[*]) created by getOrCreate at /var/folders/ht/5d19w4p161dgmlf2yzfbz7qh0000gn/T/ipykernel_74229/2060002511.py:7 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Step 1: Initialize Spark Context\n",
    "conf = SparkConf().setAppName(\"Broadcast Variable Example\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "try:\n",
    "    # Step 2: Create and Broadcast the Lookup Table\n",
    "    country_lookup = {\n",
    "        \"US\": \"United States\",\n",
    "        \"IN\": \"India\",\n",
    "        \"CA\": \"Canada\",\n",
    "        \"GB\": \"United Kingdom\",\n",
    "        \"DE\": \"Germany\"\n",
    "    }\n",
    "    broadcast_country_lookup = sc.broadcast(country_lookup)\n",
    "\n",
    "    # Step 3: Create an RDD of User Records\n",
    "    user_records = sc.parallelize([\n",
    "        {\"id\": 1, \"name\": \"John Doe\", \"country_code\": \"US\"},\n",
    "        {\"id\": 2, \"name\": \"Jane Smith\", \"country_code\": \"GB\"},\n",
    "        {\"id\": 3, \"name\": \"Sam Brown\", \"country_code\": \"CA\"},\n",
    "        {\"id\": 4, \"name\": \"Lisa Ray\", \"country_code\": \"IN\"},\n",
    "        {\"id\": 5, \"name\": \"Anne Frank\", \"country_code\": \"DE\"}\n",
    "    ])\n",
    "\n",
    "    # Step 4: Use the Broadcast Variable to Enrich User Records\n",
    "    def enrich_with_country_name(record):\n",
    "        country_code = record[\"country_code\"]\n",
    "        country_name = broadcast_country_lookup.value.get(country_code, \"Unknown\")\n",
    "        record[\"country_name\"] = country_name\n",
    "        return record\n",
    "\n",
    "    enriched_records = user_records.map(enrich_with_country_name).collect()\n",
    "\n",
    "    # Print the Enriched Records\n",
    "    for record in enriched_records:\n",
    "        print(record)\n",
    "\n",
    "finally:\n",
    "    # Stop the SparkContext\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86c7d35d-8c32-4ec1-84f7-963ec3f5684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "SparkSession$ does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython Spark SQL basic example\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/vikashraj/PycharmProjects/pyspark/devl/example1/src/main/python/library/postgresql-42.7.3.jar\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc:postgresql://localhost:5432/postgres\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.postgresql.Driver\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     18\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/session.py:503\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(session\u001b[38;5;241m.\u001b[39m_jvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession$\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1722\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: SparkSession$ does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", \"/Users/vikashraj/PycharmProjects/pyspark/devl/example1/src/main/python/library/postgresql-42.7.3.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "    .option(\"dbtable\", \"employee\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"ADMIN\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b042894-9a33-4c58-aad7-ac48b4d3b599",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o156.load.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython Spark SQL basic example\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/vikashraj/PycharmProjects/pyspark/devl/example1/src/main/python/library/postgresql-42.7.3.jar\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc:postgresql://localhost:5432/postgres\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memploye\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADMIN\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.postgresql.Driver\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     18\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     20\u001b[0m df\u001b[38;5;241m.\u001b[39mwhere(df\u001b[38;5;241m.\u001b[39memployee_id\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m103\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/readwriter.py:314\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o156.load.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", \"/Users/vikashraj/PycharmProjects/pyspark/devl/example1/src/main/python/library/postgresql-42.7.3.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\") \\\n",
    "    .option(\"dbtable\", \"employe\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"ADMIN\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.where(df.employee_id>103).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "082d1193-df14-474d-8daa-caa68018c3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/15 17:31:48 WARN Utils: Your hostname, Vikashs-Laptop.local resolves to a loopback address: 127.0.0.1, but we couldn't find any external IP address!\n",
      "24/06/15 17:31:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/15 17:31:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 1\n",
      "Scala,: 1\n",
      "and: 4\n",
      "of: 1\n",
      "machine: 1\n",
      "Apache: 1\n",
      "is: 1\n",
      "large-scale: 1\n",
      "high-level: 1\n",
      "in: 1\n",
      "Java,: 1\n",
      "that: 1\n",
      "including: 1\n",
      "an: 1\n",
      "general: 1\n",
      "learning,: 1\n",
      "GraphX: 1\n",
      "graph: 1\n",
      "processing,: 1\n",
      "a: 2\n",
      "unified: 1\n",
      "engine: 2\n",
      "provides: 1\n",
      "APIs: 1\n",
      "tools: 1\n",
      "Structured: 1\n",
      "SQL: 2\n",
      "processing.: 2\n",
      "R,: 1\n",
      "execution: 1\n",
      "offers: 1\n",
      "Streaming: 1\n",
      "analytics: 1\n",
      "for: 5\n",
      "Python: 1\n",
      "optimized: 1\n",
      "supports: 1\n",
      "graphs.: 1\n",
      "set: 1\n",
      "higher-level: 1\n",
      "MLlib: 1\n",
      "Spark: 3\n",
      "data: 1\n",
      "also: 1\n",
      "rich: 1\n",
      "DataFrames,: 1\n",
      "stream: 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCountSample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample text\n",
    "sample_text = [\n",
    "    \"Apache Spark is a unified analytics engine for large-scale data processing.\",\n",
    "    \"It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.\",\n",
    "    \"Spark also offers a rich set of higher-level tools including Spark SQL for SQL and DataFrames, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.\"\n",
    "]\n",
    "\n",
    "# Create an RDD from the sample text\n",
    "text_rdd = spark.sparkContext.parallelize(sample_text)\n",
    "\n",
    "# Split each line into words\n",
    "words = text_rdd.flatMap(lambda line: line.split())\n",
    "\n",
    "# Map each word to a (word, 1) pair\n",
    "word_pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "# Reduce by key to count the occurrences of each word\n",
    "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect the word counts to the driver\n",
    "word_counts_collected = word_counts.collect()\n",
    "\n",
    "# Print the occurrences of each word\n",
    "for word, count in word_counts_collected:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a0a452f-e9d5-4b44-bea2-1d36c63f6240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 1\n",
      "Scala,: 1\n",
      "and: 4\n",
      "of: 1\n",
      "machine: 1\n",
      "Apache: 1\n",
      "is: 1\n",
      "large-scale: 1\n",
      "high-level: 1\n",
      "in: 1\n",
      "Java,: 1\n",
      "that: 1\n",
      "including: 1\n",
      "an: 1\n",
      "general: 1\n",
      "learning,: 1\n",
      "GraphX: 1\n",
      "graph: 1\n",
      "processing,: 1\n",
      "a: 2\n",
      "unified: 1\n",
      "engine: 2\n",
      "provides: 1\n",
      "APIs: 1\n",
      "tools: 1\n",
      "Structured: 1\n",
      "SQL: 2\n",
      "processing.: 2\n",
      "R,: 1\n",
      "execution: 1\n",
      "offers: 1\n",
      "Streaming: 1\n",
      "analytics: 1\n",
      "for: 5\n",
      "Python: 1\n",
      "optimized: 1\n",
      "supports: 1\n",
      "graphs.: 1\n",
      "set: 1\n",
      "higher-level: 1\n",
      "MLlib: 1\n",
      "Spark: 3\n",
      "data: 1\n",
      "also: 1\n",
      "rich: 1\n",
      "DataFrames,: 1\n",
      "stream: 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCountSample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample text\n",
    "sample_text = [\n",
    "    \"Apache Spark is a unified analytics engine for large-scale data processing.\",\n",
    "    \"It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.\",\n",
    "    \"Spark also offers a rich set of higher-level tools including Spark SQL for SQL and DataFrames, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.\"\n",
    "]\n",
    "\n",
    "# Create an RDD from the sample text\n",
    "text_rdd = spark.sparkContext.parallelize(sample_text)\n",
    "\n",
    "# Define a function to split a line into words\n",
    "def split_line(line):\n",
    "    return line.split()\n",
    "\n",
    "# Define a function to map a word to a (word, 1) pair\n",
    "def map_word(word):\n",
    "    return (word, 1)\n",
    "\n",
    "# Define a function to reduce by key\n",
    "def reduce_counts(a, b):\n",
    "    return a + b\n",
    "\n",
    "# Split each line into words using the split_line function\n",
    "words = text_rdd.flatMap(split_line)\n",
    "\n",
    "# Map each word to a (word, 1) pair using the map_word function\n",
    "word_pairs = words.map(map_word)\n",
    "\n",
    "# Reduce by key to count the occurrences of each word using the reduce_counts function\n",
    "word_counts = word_pairs.reduceByKey(reduce_counts)\n",
    "\n",
    "# Collect the word counts to the driver\n",
    "word_counts_collected = word_counts.collect()\n",
    "\n",
    "# Print the occurrences of each word\n",
    "for word, count in word_counts_collected:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fca0717-d2a3-49bd-84ab-7766f2ef1867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------------+---------------------+\n",
      "|Age         |City                            |Name                 |\n",
      "+------------+--------------------------------+---------------------+\n",
      "|[25, 30, 35]|[New York, Los Angeles, Chicago]|[Alice, Bob, Charlie]|\n",
      "+------------+--------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create DataFrame from Dictionary\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago']\n",
    "}\n",
    "\n",
    "# Create DataFrame directly from dictionary\n",
    "df = spark.createDataFrame([data])\n",
    "\n",
    "# Show DataFrame\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f49c14b3-b399-4230-a8a7-e556eedaccb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----------+\n",
      "|Name   |Age|City       |\n",
      "+-------+---+-----------+\n",
      "|Alice  |25 |New York   |\n",
      "|Bob    |30 |Los Angeles|\n",
      "|Charlie|35 |Chicago    |\n",
      "+-------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create DataFrame from Dictionary\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago']\n",
    "}\n",
    "\n",
    "# Convert dictionary to list of tuples and create DataFrame\n",
    "data_tuples = list(zip(*data.values()))\n",
    "columns = list(data.keys())\n",
    "df = spark.createDataFrame(data_tuples, schema=columns)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a41ba749-4d96-4267-af72-bea5c5345776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----+----------+------+\n",
      "|_corrupt_record|description|  id|     movie|rating|\n",
      "+---------------+-----------+----+----------+------+\n",
      "|              [|       NULL|NULL|      NULL|  NULL|\n",
      "|           NULL|   great 3D|   1|       War|   8.9|\n",
      "|           NULL|    fiction|   2|   Science|   8.5|\n",
      "|           NULL|     boring|   3|     Irish|   6.2|\n",
      "|           NULL|    Fantasy|   4|  Ice song|   8.6|\n",
      "|           NULL|Interesting|   5|House card|   9.1|\n",
      "|              ]|       NULL|NULL|      NULL|  NULL|\n",
      "+---------------+-----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"JSONtoDataFrame\").getOrCreate()\n",
    "\n",
    "# JSON data as a string\n",
    "json_data = '''\n",
    "[\n",
    "    {\"id\": 1, \"movie\": \"War\", \"description\": \"great 3D\", \"rating\": 8.9},\n",
    "    {\"id\": 2, \"movie\": \"Science\", \"description\": \"fiction\", \"rating\": 8.5},\n",
    "    {\"id\": 3, \"movie\": \"Irish\", \"description\": \"boring\", \"rating\": 6.2},\n",
    "    {\"id\": 4, \"movie\": \"Ice song\", \"description\": \"Fantasy\", \"rating\": 8.6},\n",
    "    {\"id\": 5, \"movie\": \"House card\", \"description\": \"Interesting\", \"rating\": 9.1}\n",
    "]\n",
    "'''\n",
    "\n",
    "# Write JSON data to a temporary file\n",
    "with open('/tmp/data.json', 'w') as f:\n",
    "    f.write(json_data)\n",
    "\n",
    "# Reading JSON file into a DataFrame\n",
    "df = spark.read.json('/tmp/data.json')\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c189790-89dd-4690-a6a7-fe783665d130",
   "metadata": {},
   "source": [
    "Selecting Specific Columns\n",
    "selected_df = df.select(\"id\", \"movie\", \"rating\")\n",
    "selected_df.show()\n",
    "\n",
    "Applying a WHERE Clause\n",
    "filtered_df = df.where(df.rating > 8)\n",
    "\n",
    "\n",
    "Grouping by a Column and Applying Aggregation Functions\n",
    "grouped_df = df.groupBy(\"year\").agg({\"rating\": \"avg\"}).withColumnRenamed(\"avg(rating)\", \"average_rating\")\n",
    "\n",
    "Ordering Results\n",
    "ordered_df = df.orderBy(\"rating\")\n",
    "\n",
    "\n",
    "Limiting the Number of Results\n",
    "limited_df = df.orderBy(df.rating.desc()).limit(3)\n",
    "\n",
    "\n",
    "Using DESC for Descending Order\n",
    "desc_ordered_df = df.orderBy(df.rating.desc())\n",
    "\n",
    "\n",
    "Using MAX and MIN Functions\n",
    "from pyspark.sql.functions import max, min\n",
    "max_rating = df.select(max(\"rating\")).collect()[0][0]\n",
    "min_rating = df.select(min(\"rating\")).collect()[0][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46bb1e9c-8e55-46d8-b016-5705e1c66b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/16 00:12:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+\n",
      "| id|     movie|rating|\n",
      "+---+----------+------+\n",
      "|  1|       War|   8.9|\n",
      "|  2|   Science|   8.5|\n",
      "|  3|     Irish|   6.2|\n",
      "|  4|  Ice song|   8.6|\n",
      "|  5|House card|   9.1|\n",
      "+---+----------+------+\n",
      "\n",
      "+-----------+---+----------+------+----+\n",
      "|description| id|     movie|rating|year|\n",
      "+-----------+---+----------+------+----+\n",
      "|   great 3D|  1|       War|   8.9|2020|\n",
      "|    fiction|  2|   Science|   8.5|2021|\n",
      "|    Fantasy|  4|  Ice song|   8.6|2022|\n",
      "|Interesting|  5|House card|   9.1|2018|\n",
      "+-----------+---+----------+------+----+\n",
      "\n",
      "+----+--------------+\n",
      "|year|average_rating|\n",
      "+----+--------------+\n",
      "|2020|           8.9|\n",
      "|2021|           8.5|\n",
      "|2019|           6.2|\n",
      "|2022|           8.6|\n",
      "|2018|           9.1|\n",
      "+----+--------------+\n",
      "\n",
      "+-----------+---+----------+------+----+\n",
      "|description| id|     movie|rating|year|\n",
      "+-----------+---+----------+------+----+\n",
      "|     boring|  3|     Irish|   6.2|2019|\n",
      "|    fiction|  2|   Science|   8.5|2021|\n",
      "|    Fantasy|  4|  Ice song|   8.6|2022|\n",
      "|   great 3D|  1|       War|   8.9|2020|\n",
      "|Interesting|  5|House card|   9.1|2018|\n",
      "+-----------+---+----------+------+----+\n",
      "\n",
      "+-----------+---+----------+------+----+\n",
      "|description| id|     movie|rating|year|\n",
      "+-----------+---+----------+------+----+\n",
      "|Interesting|  5|House card|   9.1|2018|\n",
      "|   great 3D|  1|       War|   8.9|2020|\n",
      "|    Fantasy|  4|  Ice song|   8.6|2022|\n",
      "+-----------+---+----------+------+----+\n",
      "\n",
      "+-----------+---+----------+------+----+\n",
      "|description| id|     movie|rating|year|\n",
      "+-----------+---+----------+------+----+\n",
      "|Interesting|  5|House card|   9.1|2018|\n",
      "|   great 3D|  1|       War|   8.9|2020|\n",
      "|    Fantasy|  4|  Ice song|   8.6|2022|\n",
      "|    fiction|  2|   Science|   8.5|2021|\n",
      "|     boring|  3|     Irish|   6.2|2019|\n",
      "+-----------+---+----------+------+----+\n",
      "\n",
      "Max Rating: 9.1\n",
      "Min Rating: 6.2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max, min\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DetailedExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    {\"id\": 1, \"movie\": \"War\", \"description\": \"great 3D\", \"rating\": 8.9, \"year\": 2020},\n",
    "    {\"id\": 2, \"movie\": \"Science\", \"description\": \"fiction\", \"rating\": 8.5, \"year\": 2021},\n",
    "    {\"id\": 3, \"movie\": \"Irish\", \"description\": \"boring\", \"rating\": 6.2, \"year\": 2019},\n",
    "    {\"id\": 4, \"movie\": \"Ice song\", \"description\": \"Fantasy\", \"rating\": 8.6, \"year\": 2022},\n",
    "    {\"id\": 5, \"movie\": \"House card\", \"description\": \"Interesting\", \"rating\": 9.1, \"year\": 2018}\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# 1. Select specific columns\n",
    "selected_df = df.select(\"id\", \"movie\", \"rating\")\n",
    "selected_df.show()\n",
    "\n",
    "# 2. Apply WHERE clause\n",
    "filtered_df = df.where(df.rating > 8)\n",
    "filtered_df.show()\n",
    "\n",
    "# 3. Group by column and apply aggregation functions\n",
    "grouped_df = df.groupBy(\"year\").agg({\"rating\": \"avg\"}).withColumnRenamed(\"avg(rating)\", \"average_rating\")\n",
    "grouped_df.show()\n",
    "\n",
    "# 4. Order results\n",
    "ordered_df = df.orderBy(\"rating\")\n",
    "ordered_df.show()\n",
    "\n",
    "# 5. Limit the number of results\n",
    "limited_df = df.orderBy(df.rating.desc()).limit(3)\n",
    "limited_df.show()\n",
    "\n",
    "# 6. Order by descending\n",
    "desc_ordered_df = df.orderBy(df.rating.desc())\n",
    "desc_ordered_df.show()\n",
    "\n",
    "# 7. Use MAX and MIN functions\n",
    "max_rating = df.select(max(\"rating\")).collect()[0][0]\n",
    "min_rating = df.select(min(\"rating\")).collect()[0][0]\n",
    "\n",
    "print(f\"Max Rating: {max_rating}\")\n",
    "print(f\"Min Rating: {min_rating}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d44739-6a8d-4914-90b0-06d19aff458c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
